{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost(Extreme Gradient Boosting)\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library. Yes, it uses gradient boosting (GBM) framework at core. Yet, does better than GBM framework alone. XGBoost was created by Tianqi Chen, PhD Student, University of Washington. It is used for supervised ML problems.  Let's look at what makes it so good:\n",
    "\n",
    "#### 1.Parallel Computing:\n",
    "It is enabled with parallel processing (using OpenMP); i.e., when you run xgboost, by default, it would use all the cores of your laptop/machine.\n",
    "\n",
    "#### 2.Regularization:\n",
    "I believe this is the biggest advantage of xgboost. GBM has no provision for regularization. Regularization is a technique used to avoid overfitting in linear and tree-based models.\n",
    "\n",
    "#### 3.Enabled Cross Validation:\n",
    "In R, we usually use external packages such as caret and mlr to obtain CV results. But, xgboost is enabled with internal CV function (we'll see below).\n",
    "\n",
    "#### 4.Missing Values:\n",
    "XGBoost is designed to handle missing values internally. The missing values are treated in such a manner that if there exists any trend in missing values, it is captured by the model.\n",
    "\n",
    "#### 5.Flexibility:\n",
    "In addition to regression, classification, and ranking problems, it supports user-defined objective functions also. An objective function is used to measure the performance of the model given a certain set of parameters. Furthermore, it supports user defined evaluation metrics as well.\n",
    "\n",
    "#### 6.Availability:\n",
    "Currently, it is available for programming languages such as R, Python, Java, Julia, and Scala.\n",
    "\n",
    "#### 7.Save and Reload:\n",
    "XGBoost gives us a feature to save our data matrix and model and reload it later. Suppose, we have a large data set, we can simply save the model and use it in future instead of wasting time redoing the computation.\n",
    "\n",
    "#### 8.Tree Pruning:\n",
    "Unlike GBM, where tree pruning stops once a negative loss is encountered, XGBoost grows the tree upto max_depth and then prune backward until the improvement in loss function is below a threshold.\n",
    "\n",
    "### Implementation of XGBoost is given below on the data set\n",
    "### >> Import all useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>check data after reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  sex  chest  resting_blood_pressure  serum_cholestoral  \\\n",
      "0  70.0  1.0    4.0                   130.0              322.0   \n",
      "1  67.0  0.0    3.0                   115.0              564.0   \n",
      "2  57.0  1.0    2.0                   124.0              261.0   \n",
      "3  64.0  1.0    4.0                   128.0              263.0   \n",
      "4  74.0  0.0    2.0                   120.0              269.0   \n",
      "\n",
      "   fasting_blood_sugar  resting_electrocardiographic_results  \\\n",
      "0                  0.0                                   2.0   \n",
      "1                  0.0                                   2.0   \n",
      "2                  0.0                                   0.0   \n",
      "3                  0.0                                   0.0   \n",
      "4                  0.0                                   2.0   \n",
      "\n",
      "   maximum_heart_rate_achieved  exercise_induced_angina  oldpeak  slope  \\\n",
      "0                        109.0                      0.0      2.4    2.0   \n",
      "1                        160.0                      0.0      1.6    2.0   \n",
      "2                        141.0                      0.0      0.3    1.0   \n",
      "3                        105.0                      1.0      0.2    2.0   \n",
      "4                        121.0                      1.0      0.2    1.0   \n",
      "\n",
      "   number_of_major_vessels  thal  class  \n",
      "0                      3.0   3.0      1  \n",
      "1                      0.0   7.0      0  \n",
      "2                      0.0   7.0      1  \n",
      "3                      1.0   7.0      0  \n",
      "4                      1.0   3.0      0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>Take input features and target feature in different variables respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[130. 322.   0. ...   2.   3.   3.]\n",
      " [115. 564.   0. ...   2.   0.   7.]\n",
      " [124. 261.   0. ...   1.   0.   7.]\n",
      " ...\n",
      " [140. 294.   0. ...   2.   0.   3.]\n",
      " [140. 192.   0. ...   2.   0.   6.]\n",
      " [160. 286.   0. ...   2.   3.   3.]]\n"
     ]
    }
   ],
   "source": [
    "x=df.iloc[:,3:13].values\n",
    "y = df.iloc[:, 13].values   #target feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>Take training and testing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of XGBoost\n",
    "#### Booster[default=gbtree]\n",
    "Sets the booster type (gbtree, gblinear or dart) to use. For classification problems, you can use gbtree, dart. For regression, you can use any.\n",
    "#### nthread[default=maximum cores available]\n",
    "Activates parallel computation. Generally, people don't change it as using maximum cores leads to the fastest computation.\n",
    "#### silent[default=0]\n",
    "If you set it to 1, your R console will get flooded with running messages. Better not to change it.\n",
    "#### nrounds[default=100]\n",
    "It controls the maximum number of iterations. For classification, it is similar to the number of trees to grow.\n",
    "Should be tuned using CV\n",
    "#### eta[default=0.3][range: (0,1)]\n",
    "It controls the learning rate, i.e., the rate at which our model learns patterns in data. After every round, it shrinks the feature weights to reach the best optimum.\n",
    "Lower eta leads to slower computation. It must be supported by increase in nrounds.\n",
    "Typically, it lies between 0.01 - 0.3\n",
    "#### gamma[default=0][range: (0,Inf)]\n",
    "It controls regularization (or prevents overfitting). The optimal value of gamma depends on the data set and other parameter values.\n",
    "Higher the value, higher the regularization. Regularization means penalizing large coefficients which don't improve the model's performance. default = 0 means no regularization.\n",
    "#### Tune trick:\n",
    "Start with 0 and check CV error rate. If you see train error >>> test error, bring gamma into action. Higher the gamma, lower the difference in train and test CV. If you have no clue what value to use, use gamma=5 and see the performance. Remember that gamma brings improvement when you want to use shallow (low max_depth) trees.\n",
    "#### max_depth[default=6][range: (0,Inf)]\n",
    "It controls the depth of the tree.\n",
    "Larger the depth, more complex the model; higher chances of overfitting. There is no standard value for max_depth. Larger data sets require deep trees to learn the rules from data.\n",
    "Should be tuned using CV\n",
    "#### min_child_weight[default=1][range:(0,Inf)]\n",
    "In regression, it refers to the minimum number of instances required in a child node. In classification, if the leaf node has a minimum sum of instance weight (calculated by second order partial derivative) lower than min_child_weight, the tree splitting stops.\n",
    "In simple words, it blocks the potential feature interactions to prevent overfitting. Should be tuned using CV.\n",
    "#### subsample[default=1][range: (0,1)]\n",
    "It controls the number of samples (observations) supplied to a tree.\n",
    "Typically, its values lie between (0.5-0.8)\n",
    "#### colsample_bytree[default=1][range: (0,1)]\n",
    "It control the number of features (variables) supplied to a tree\n",
    "Typically, its values lie between (0.5,0.9)\n",
    "#### lambda[default=0]\n",
    "It controls L2 regularization (equivalent to Ridge regression) on weights. It is used to avoid overfitting.\n",
    "#### alpha[default=1]\n",
    "It controls L1 regularization (equivalent to Lasso regression) on weights. In addition to shrinkage, enabling alpha also results in feature selection. Hence, it's more useful on high dimensional data sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>Apply XGBClassifier now and check what happen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
      " 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "output=xgb.XGBClassifier(max_depth=7,\n",
    "                           min_child_weight=1,\n",
    "                           learning_rate=0.1,\n",
    "                           n_estimators=500,\n",
    "                           silent=True,\n",
    "                           objective='binary:logistic',\n",
    "                           gamma=0,\n",
    "                           max_delta_step=0,\n",
    "                           subsample=1,\n",
    "                           reg_alpha=0,\n",
    "                           reg_lambda=0,\n",
    "                           scale_pos_weight=1,\n",
    "                           seed=1,\n",
    "                           missing=None)\n",
    "output.fit(x_train,y_train, eval_metric='auc')\n",
    "y_pred=output.predict(x_test)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>Lets check accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Infinite Solutions LLP\n",
    "\n",
    "by [Research Infinite Solutions](http://www.researchinfinitesolutions.com/)\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
